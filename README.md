<a href="https://chat.vercel.ai/">
  <h1 align="center">SyDeTre - Medical Chatbot Interface</h1>
</a>

<p align="center">
  An open-source medical chatbot interface built with Next.js, for a 7B Mistral fine-tuned model for symptom detection and treatment recommendations.
</p>

<p align="center">
  <a href="#features"><strong>Features</strong></a> ·
  <a href="#dataset-and-model-training"><strong>Dataset and Model Training</strong></a> ·
  <a href="#getting-started"><strong>Getting Started</strong></a> ·
  <a href="#project-structure"><strong>Project Structure</strong></a> ·
  <a href="#fine-tuning-process"><strong>Fine-Tuning Process</strong></a> ·</p>
<br/>

## Capabilties

- Symptom detection
- Treatment recommendations
- General medicine knowledge

## Dataset and Model Training

The dataset used for fine-tuning SyDeTre is a combination of:
- Data generated by querying the Open Mixtral 8x22B model with questions about pharmaceutical drugs and illnesses.
- Various MIT license datasets related to medical information.

Below is the final huggingface dataset:https://huggingface.co/datasets/toniz/sydetre-72k-drugs-treatments/settings

The fine-tuning process was conducted using the Mistral fine-tuning API.

## WandB results

The finetuning results can be found here: 
![Train](public/WANDBTRAIN.jpg)
![Eval](public/WANDBEVAL.jpg)

## Getting Started

### Prerequisites
- Node.js (v14 or above)
- Yarn or npm

### Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/yourusername/sydetre.git
    cd sydetre
    ```

2. Install dependencies:
    ```bash
    yarn install
    # or
    npm install
    ```

3. Set up environment variables:
    Create a `.env` file in the root directory and add the necessary configuration variables as in the .env.example.
4. Start the development server:
    ```bash
    yarn dev
    # or
    npm run dev
    ```

    The application will be available at `http://localhost:3000`.

## Fine-Tuning Process

The fine-tuning process for SyDeTre involves using the Mistral fine-tuning API with a combination of generated and MIT license datasets. The `finetune/` folder includes all necessary scripts and configuration files for replicating the fine-tuning process.


## License

This project is licensed under the MIT License.
